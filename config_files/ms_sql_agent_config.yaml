extensions:
  health_check:
    endpoint: "${SPLUNK_LISTEN_INTERFACE}:13133"
  http_forwarder:
    ingress:
      endpoint: "${SPLUNK_LISTEN_INTERFACE}:6060"
    egress:
      # endpoint: "${SPLUNK_API_URL}"
      # Use instead when sending to gateway
      endpoint: "${SPLUNK_GATEWAY_URL}"
  smartagent:
    bundleDir: "${SPLUNK_BUNDLE_DIR}"
    # collectd:
      # configDir: "${SPLUNK_COLLECTD_DIR}"
  zpages:
    endpoint: "${SPLUNK_LISTEN_INTERFACE}:55679"

receivers:
  hostmetrics:
    collection_interval: 1s
    scrapers:
      cpu:
      disk:
      filesystem:
      memory:
      network:
      # System load average metrics https://en.wikipedia.org/wiki/Load_(computing)
      load:
      # Paging/Swap space utilization and I/O metrics
      paging:
      # Aggregated system process count metrics
      processes:
      # System processes metrics, disabled by default
      # process:

  otlp:
    protocols:
      grpc:
        endpoint: "${SPLUNK_LISTEN_INTERFACE}:4317"
      http:
        endpoint: "${SPLUNK_LISTEN_INTERFACE}:4318"

  # This section is used to collect the OpenTelemetry Collector metrics
  # Even if just a Splunk APM customer, these metrics are included
  prometheus/internal:
    config:
      scrape_configs:
      - job_name: 'otel-collector'
        scrape_interval: 10s
        static_configs:
        - targets: ["0.0.0.0:8888"]
        metric_relabel_configs:
          - source_labels: [ __name__ ]
            regex: 'otelcol_rpc_.*'
            action: drop
          - source_labels: [ __name__ ]
            regex: 'otelcol_http_.*'
            action: drop
          - source_labels: [ __name__ ]
            regex: 'otelcol_processor_batch_.*'
            action: drop

  smartagent/processlist:
    type: processlist
  
  signalfx:
    endpoint: "${SPLUNK_LISTEN_INTERFACE}:9943"
    # Whether to preserve incoming access token and use instead of exporter token
    # default = false
    #access_token_passthrough: true

  nop:

  sqlserver:
    collection_interval: 5s
    username: ${SPLUNK_SQL_USER}
    password: ${SPLUNK_SQL_USER_PWD}
    server: 0.0.0.0
    port: 1433
    resource_attributes:
      sqlserver.instance.name:
        enabled: true

  smartagent/sqlserver:
    type: telegraf/sqlserver
    host: localhost
    port: 1433
    userID: ${SPLUNK_SQL_USER}
    password: ${SPLUNK_SQL_USER_PWD}
    appName: sqlserver
    extraMetrics:
      - sqlserver_memory_clerks.size_kb.bound_trees
      - sqlserver_performance.active_temp_tables


  smartagent/win_services:
    type: telegraf/win_services
    intervalSeconds: 1
    serviceNames:
      - Dnscache
      - Dhcp
      - LanmanServer
      - LanmanWorkstation
      - SessionEnv
      - EventLog
      - lmhosts
      - PlugPlay
      - RpcEptMapper
      - MSDTC
      - SamSs
      - MSSQLSERVER
      - SQLSERVERAGENT

  smartagent/procstat-otel:
    type: telegraf/procstat
    intervalSeconds: 1
    exe: otelcol.exe
    pattern: otel
    WinService: splunk-otel-collector

  smartagent/procstat-mssqlserver:
    type: telegraf/procstat
    intervalSeconds: 1
    exe: sqlservr.exe
    pattern: MSSQLSERVER
    WinService: MSSQLSERVER

  # smartagent/telegraf/win_perf_counters:
  #   type: telegraf/win_perf_counters
  #   printValid: true
  #   counterRefreshInterval: 10
  #   objects:
  #    - objectName: "Processor"
  #      instances:
  #       - "*"
  #      counters:
  #       - "% Idle Time"
  #       - "% Interrupt Time"
  #       - "% Privileged Time"
  #       - "% User Time"
  #       - "% Processor Time"
  #      includeTotal: true
  #      measurement: "win_cpu"

  #    - objectName: "PhysicalDisk"
  #      instances:
  #       - "*"
  #      counters:
  #       - "% Disk Read Time"
  #       - "% Disk Write Time"
  #       - "% Idle Time"
  #      includeTotal: false
  #      measurement: "win_disk"

# Replaced SmartAgent with OTel Perfomn Receivers
# https://docs.splunk.com/observability/en/gdi/opentelemetry/components/windowsperfcounters-receiver.html#windows-performance-counters-receiver
  windowsperfcounters/memory:
    metrics:
      bytes.committed:
        description: Number of bytes committed to memory
        unit: By
        gauge:
    collection_interval: 10s
    perfcounters:
      - object: Memory
        counters:
          - name: Committed Bytes
            metric: bytes.committed

  windowsperfcounters/processor:
    collection_interval: 5s
    metrics:
      processor.time:
        description: CPU active and idle time
        unit: "%"
        gauge:
    perfcounters:
      - object: "Processor"
        instances: "*"
        counters:
          - name: "% Processor Time"
            metric: processor.time
            attributes:
              state: active
      - object: "Processor"
        instances: ["0", "1", "2", "3"]
        counters:
          - name: "% Idle Time"
            metric: processor.time
            attributes:
              state: idle

  smartagent/ntp:
    type: ntp
    host: pool.ntp.org

processors:
  batch:
    metadata_keys:
      - X-SF-Token

  # Enabling the memory_limiter is strongly recommended for every pipeline.
  # Configuration is based on the amount of memory allocated to the collector.
  # For more information about memory limiter, see
  # https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiter/README.md
  memory_limiter:
    check_interval: 2s
    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}

  # Detect if the collector is running on a cloud system, which is important for creating unique cloud provider dimensions.
  # Detector order is important: the `system` detector goes last so it can't preclude cloud detectors from setting host/os info.
  # Resource detection processor is configured to override all host and cloud attributes because instrumentation
  # libraries can send wrong values from container environments.
  # https://docs.splunk.com/Observability/gdi/opentelemetry/components/resourcedetection-processor.html#ordering-considerations
  resourcedetection:
    detectors: [gcp, ecs, ec2, azure, system]
    override: true

  # Optional: The following processor can be used to add a default "deployment.environment" attribute to the logs and
  # traces when it's not populated by instrumentation libraries.
  # If enabled, make sure to enable this processor in a pipeline.
  # For more information, see https://docs.splunk.com/Observability/gdi/opentelemetry/components/resource-processor.html
  #resource/add_environment:
    #attributes:
      #- action: insert
        #value: staging/production/...
        #key: deployment.environment

  # The following processor is used to add "otelcol.service.mode" attribute to the internal metrics
  resource/add_mode:
    attributes:
      - action: insert
        value: "agent"
        key: otelcol.service.mode

exporters:
  # Traces
  otlphttp:
    traces_endpoint: "${SPLUNK_INGEST_URL}/v2/trace/otlp"
    headers:
      "X-SF-Token": "${SPLUNK_ACCESS_TOKEN}"

  # Metrics + Events
  signalfx:
    access_token: "${SPLUNK_ACCESS_TOKEN}"
    # api_url: "${SPLUNK_API_URL}"
    # ingest_url: "${SPLUNK_INGEST_URL}"
    # Use instead when sending to gateway
    api_url: http://${SPLUNK_GATEWAY_URL}:6060
    ingest_url: http://${SPLUNK_GATEWAY_URL}:9943
    sync_host_metadata: true
    correlation:

  # Entities (applicable only if discovery mode is enabled)
  otlphttp/entities:
    logs_endpoint: "${SPLUNK_INGEST_URL}/v3/event"
    headers:
      "X-SF-Token": "${SPLUNK_ACCESS_TOKEN}"

  # Logs
  splunk_hec:
    token: "${SPLUNK_HEC_TOKEN}"
    endpoint: "${SPLUNK_HEC_URL}"
    source: "otel"
    sourcetype: "otel"
    profiling_data_enabled: false

  # Profiling
  splunk_hec/profiling:
    token: "${SPLUNK_ACCESS_TOKEN}"
    endpoint: "${SPLUNK_INGEST_URL}/v1/log"
    log_data_enabled: false

  # Send to gateway
  otlphttp/gateway:
    endpoint: http://${SPLUNK_GATEWAY_URL}:4318
  otlp/gateway:
    endpoint: "${SPLUNK_GATEWAY_URL}:4317"
    tls:
      insecure: true

  # Debug
  debug:
    verbosity: detailed

service:
  telemetry:
    metrics:
      address: "${SPLUNK_LISTEN_INTERFACE}:8888"

  extensions:
  - health_check
  - http_forwarder
  - zpages
  - smartagent

  pipelines:
    traces:
      receivers:
      - otlp
      processors:
      - memory_limiter
      - batch
      - resourcedetection
      #- resource/add_environment
      exporters:
      # - otlphttp
      # - signalfx
      # Use instead when sending to gateway
      - otlphttp/gateway
      - signalfx

    metrics:
      receivers:
      - hostmetrics
      - otlp
      - signalfx
      - sqlserver
      - smartagent/sqlserver
      - smartagent/win_services
      - smartagent/procstat-otel
      - smartagent/procstat-mssqlserver
      - smartagent/ntp
      - windowsperfcounters/memory
      - windowsperfcounters/processor
      processors:
      - memory_limiter
      - batch
      - resourcedetection
      exporters:
      # - signalfx
      # Use instead when sending to gateway
      - otlphttp/gateway

    metrics/internal:
      receivers:
      - prometheus/internal
      processors:
      - memory_limiter
      - batch
      - resourcedetection
      - resource/add_mode
      # When sending to gateway, at least one metrics pipeline needs
      # to use signalfx exporter so host metadata gets emitted
      exporters:
      - signalfx

    logs/signalfx:
      receivers:
      - signalfx
      - smartagent/processlist
      processors:
      - memory_limiter
      - batch
      - resourcedetection
      exporters:
      - signalfx

    logs/entities:
      # Receivers are dynamically added if discovery mode is enabled
      receivers:
      - nop
      processors:
      - memory_limiter
      - batch
      - resourcedetection
      exporters:
      - otlphttp/entities

    logs:
      receivers:
      - otlp
      processors:
      - memory_limiter
      - batch
      - resourcedetection
      #- resource/add_environment
      exporters:
      # - splunk_hec
      # - splunk_hec/profiling
      # Use instead when sending to gateway
      - otlphttp/gateway